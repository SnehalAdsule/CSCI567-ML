\documentclass[10pt,letterpaper]{article}
\usepackage{amsmath}

\begin{document}
\title{CSCI 567 Assignment 2 \\Fall 2016}
\date{ October 3,2016}
\author{Snehal Adsule\\2080872073\\adsule@usc.edu}
\maketitle

\newpage
%---- PROBLEM 1
\section{Logistic Regression }
\subsection{1 (a)}
Given n training examples $(x_1,y_1),(x_2,y_2) .. (x_n,y_n)$ and binary logistic regression as
	$L(w)=-\log\big(\prod_{i=1}^n P(Y=y_1|X=x_i)\big)$\\
$P(y_i|x_i,b,w)=\sigma(b+w^T x_n)^y_n \times (1- \sigma(b+w^T x_n)^{1-y_n}) $\\

Taking negative log likelihood, we have 
\begin{align*}
L(w)=&-[y_n \log(\sigma(b+w^T x_n)) +(1- y_n)\log (1- \sigma(b+w^T x_n))]\tag{1}
\end{align*}

\subsection{1 (b)}
To find gradient descent of the equation from 1(a) wrt $w$\\

$\frac{dL(w)}{dw_n}=\frac{y_n\times x_n}{\sigma(b+w^T x_n)} + (\frac{(1- y_n) \times (-x_n)}{1-\sigma(b+w^T x_n)})$\\
$=y_n x_n -y_n x_n \sigma(b+w^T x_n) -x_n \sigma(b+w^T x_n) +y_n x_n \sigma(b+w^T x_n)$\\
$=-(y_n - \sigma(b+w^T x_n))x_n$\\
$\frac {dL(w)}{dw_i}= \Sigma_{i=1}^n ( \sigma(b+w^T x_i) - y_i )x_i $\\
therefore the gradient descent update rule will be \\\\
$w^{(t+1)}=w^t - \eta \Sigma_{i=1}^n ( \sigma(b+w^T x_i) - y_i )x_i$\\

As the above update rule will depend on the$ \eta$ and choosing the right step size we will converge to the stationary point for being the minimum.

\subsection{1(c)} Extending the binary logistic to the multi-class classification,\\
$P(Y=k|X=x)= \frac{exp(w_k^T x)}{1+ \Sigma_1^{K-1} exp(w_t^T x)}$ for k=1,...K-1\\
$P(Y=k|X=x)= \frac{1}{1+ \Sigma_1^{K-1} exp(w_t^T x)}$ for k=K class\\

we can denote the 1 =$exp(w_k^T)$ where $w_k=0$, to represnt it inthe folowwing form \\
$P(Y=k|X=x)=\frac{exp(w_k^Tx)}{\Sigma_{i=1}^K exp(w_t^T x)}$\\

Taking the negative log on both side we have,\\

$L(w)= - \log \prod_{i=1}^N  P(Y=k|X=x)$\\
$L(w_i)= -  \prod_{i=1}^K   \log p(k |x_i)^{y_{i}^k}$\\
$L(w_1,w_2,..w_k)= -  \Sigma_{i=1}^N \Sigma_{k=1}^K  y_{i}^k \log p(k |x_i)$\\

\subsection{1 (d)}
Taking the gradient wrt $w_k$ on the negative likelihood we get ,\\
$dL(w)/d w_i =d ((y_i^k  log \frac{exp(w^T_k x_i)}{exp(\Sigma_{i=1}^K (w_t^T x_i)})) )/dw_i$\\
$dL(w)/d w_i =d ((y_i^k  w^T_i - log( 1+\Sigma_{t=1}^{K-1}exp (w_t^T x_i)))x_i )/dw_i$\\
Consider derivative of $ \log\frac{ 1}{1+ exp(a)}$  wrt  $d(a)$\\
$=-e^a / 1+ e^a$ \\
Using the above for where a=$w_k^T x_i$ we can re write the gradient as \\
	$dL(w)/d w_i =(y_i^k - P(k|x_i))x_i$\\\\
Therefore, the update rule for the each vector $w_k=$\\
	$w_k(t+1) =  w_k(t) + (y_i^k - P(k|x_i))x_i$


%---- PROBLEM 2
\section{Linear \ Gaussian Discriminant }
\subsection{2 (a)}
Consider the likelihood function 
\begin{align*}
p(x,y)=p(y)p(x|y)=\\
=& p1\frac{1}{\sqrt{ (2\pi)}\sigma_1} e^{-\frac{(x-\mu_1)^2}{\sigma_1^2}} &for y=1 \\
& p2\frac{1}{\sqrt{ (2\pi)}\sigma_2} e^{-\frac{(x-\mu_2)^2}{\sigma_2^2}}  &for &y=2\\
\end{align*}

Likelihood of the training data D =$((x_n,y_n))_{n=1}^N$
\begin{align*}
log(P(D)) =\Sigma_n log p(x_n,y_n)\\
=&\Sigma_n \log p1\frac{1}{\sqrt{ (2\pi)}\sigma_1} e^{-\frac{(x-\mu_1)^2}{\sigma_1^2}} \\
+ &\Sigma_n \log p2\frac{1}{\sqrt{ (2\pi)}\sigma_2} e^{-\frac{(x-\mu_2)^2}{\sigma_2^2}} \\
&= \Sigma_n (\log p1 - \log\sqrt{ (2\pi)}\sigma_1 -\frac{(x-\mu_1)^2}{\sigma_1^2} \\
& \log p2 - \log\sqrt{ (2\pi)}\sigma_2 -\frac{(x-\mu_2)^2}{\sigma_2^2} )
\end{align*}

we know that p1+p2=1 is the constraint , and taking partial derivative wrt $p1, \mu_1 , \sigma_1    $\\
consider 
\begin{align*}
\Sigma_n (\log p1 ) +\Sigma_n (\log p2 )\\
= N_k log p1 + (N-N_k) log (1-p1)\\
\end{align*}
taking derivative wrt p1
\begin{align*}
= >N_k /p1 - (N-N_k) / (1-p1)  =0 \\
=>N_k(1-p1) - (N-N_k)p1 =0\\
=>N_k -N_k p1 - (N- N_k)p1 =0 => N_k = (N_k+ N -N_k) p1\\
=> \hat p1= N_k/N\\
\text{Therefore ,} => \hat p2= (N- N_k)/N
\end{align*}


Now, taking derivative wrt $\mu_1$
\begin{align*}
 \Sigma_n \frac{-2(x-\mu_1)}{\sigma_1^2} =0\\
=> \Sigma_n x_i  - N_k \mu_1=0\\
=>\hat \mu_1 =\frac {\Sigma_n x_i}{N_k} \\
\text{Therefore ,} =>& \hat \mu_2=\frac {\Sigma_n x_i} {N-N_k} 
\end{align*}

Now, taking derivative wrt $\sigma_1$
\begin{align*}
 \Sigma_n \frac{(x-\mu_1)^2}{\sigma_1^3} -\frac{1}{\sigma_1}  =0\\
=>  \Sigma_n (x-\mu_1)^2 - N_k\sigma_1^2=0\\
=>\hat\sigma_1 = \sqrt{\frac { \Sigma_n (x_i-\mu_1)^2}{N_k} }\\
\text{Therefore ,} =>& \hat \sigma_2= \sqrt{\frac {\Sigma_n (x_i-\mu_1)^2} {N-N_k} }
\end{align*}

where $N_k$ is the number observation for class 1 and   $N$ as total noumber of observation.

\subsection{2 (b)}
Given that,
$P(X|Y=c_1) \sim N(\mu_1, \Sigma)$ and $p(X|Y=c_2) \sim N(\mu_2, \Sigma)$
		
		where $\mu_1, \mu_2 \in R^D, \Sigma \in R^{D\times D}$
		
\begin{align*}
P(Y=1|X) &= \frac{P(X|Y=1)P(Y=1)}{P(X)}\\
		&= \frac{P(X|Y=1)P(Y=1)}{P(X|Y=1)P(Y=1)+P(X|Y=2)P(Y=2)}\\
		&= \frac{1}{1+\frac{P(X|Y=2)P(Y=2)}{P(X|Y=1)P(Y=1)}}\\
		&= \frac{1}{1+\exp(\log(\frac{P(X|Y=2)P(Y=2)}{P(X|Y=1)P(Y=1)}))}\\
		&= \frac{1}{1+\exp(\log({P(X|Y=2)P(Y=2)})-\log({P(X|Y=1)P(Y=1)}))}\\
		&= \frac{1}{1+\exp(-(\log(\frac{P(Y=1)}{P(Y=2)}))+\log(P(X|Y=2))-\log(P(X|Y=1)))}\\
		&=equation 1
\end{align*}
		Using the given definition to calculate the$ P(X|Y=2) ,and P(X|Y=1)$
\begin{align*}
\log(P(X|Y=1)) &= -\frac{1}{2} \ln(|\Sigma|) - \frac{D}{2}\ln(\pi) -\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\\ 
\log(P(X|Y=2)) &= -\frac{1}{2} \ln(|\Sigma|) - \frac{D}{2}\ln(\pi) -\frac{1}{2}(x-\mu_2)^T\Sigma^{-1}(x-\mu_2)\\
\log(P(X|Y=2))-\log(P(X|Y=1)) &=- \frac{1}{2}(x-\mu_2)^T\Sigma^{-1}(x-\mu_2) + \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) \\
		 &= (\mu_1^T-\mu_2^T)\Sigma^{-1}x + x^T\Sigma^{-1}(\mu_1-\mu_2)\\
		 &= 2(\mu_1^T-\mu_2^T)\Sigma^{-1}x + \mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2
\end{align*}
		
		Substituting  these values back into the equation 1:
\begin{align*}
P(Y=1|X) &= \frac{1}{1+\exp(-(\log(\frac{P(Y=1)}{P(Y=2)})+ \mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2+2(\mu_1^T-\mu_2^T)\Sigma^{-1}x  ))}\\
P(Y=1|X) &= \frac{1}{1+\exp(-(K)+\theta^T x  ))} 
\end{align*}
		
		Where $K = \frac{P(Y=1)}{P(Y=2)}- \mu_1^T \Sigma^{-1} \mu_1 + \mu_2^T \Sigma^{-1} \mu_2$ , and	Using $ (\Sigma^{-1})^T = \Sigma^{-1})$\\\\
		$\theta = 2(\mu_1-\mu_2)\Sigma^{-1}$

\newpage
\section{Programming - Linear Regression}
\subsection{ Data processing and analysis }
The data was split into training and test using the read\_data.py file as one time activity.\\
The histograms are attached in the homeword folder  $\text{figure\_1.png}$ to $\text{figure\_13.png}$ for your perusal. The data was standardize for the data processing .\\

The pearson coefficient are as follows:\\
0: -0.3876969876214702,\\ 1: 0.362987295831417,\\ 2: -0.48306742175757955,\\ 3: 0.2036001446957322,\\ 4: -0.42482967561932006,\\ 5: 0.690923334973398,\\ 6: -0.390179110400713,\\ 7: 0.25242056622498404,\\ 8: -0.3854918144230041,\\ 9: -0.4688493853729993,\\ 10: -0.505270756891863,\\ 11: 0.343434137150922,\\ 12: -0.7399698206299233

\subsection{Linear Regression}
Linear Regression and Ridge regression was performed on the boston housing data using the normal equation solution. The MSE was used for the performance on training and test dataset.\\\\
Linear Regression MSE Results:\\
$Linear MSE_{train} = 20.950145$\\
$Linear MSE_{test} = 28.398394$\\\\

Ridge Regression MSE results for $\lambda = 0.01 ,0.1, 1.0$\\
lambda  0.01 $MSE_{train}$  =   20.950145\\
lambda  0.01 $MSE_{test}$   =   28.398849\\\\
lambda  0.1 $MSE_{train}$   =   20.950157\\
lambda  0.1 $MSE_{test}$    =   28.402944\\\\
lambda  1.0 $MSE_{train}$   =   20.951318\\
lambda  1.0 $MSE_{test}$    =   28.443596\\\\

Linear Regression still showed better perfromance as comapred to the ridge regression with the given values of $\lambda$. With the increase in the value of lambda the MSE value increased.\\

Cross Validation for Ridge Regression:\\
The dataset was divided into 10 folds using the i-th logic for cross validation on the training set. Various values of $\lambda$ were tried on the dataset and the averaged over the  $\lambda$ values.
The top avergae values were:\\
Average lamdba [(0.0, 20.855763400184646), (0.0001, 20.855763400200132),...]\\

It was observed that for various fold for different value of lambda , showed significant improvement for the MSE on train set . The various 10 fold intermediate results are attached in the CV.txt for your reference. The best lamdba was chosen to evaluate the MSE on test set.\\The best lambda for training CV$=>  \lambda = 0.0$ and $MSE_{test_{CV}} =  22.9756820127$

\subsection{Feature Selection}
The following techniques were applied on the dataset .

1.  Feature selection based on absolute value of pearson coefffcient.\\ The selected features were ['LSTAT', 'RM', 'PTRATIO', 'INDUS'] in the decreasing order of the pearson coefficent .

	top 4 features $MSE_{train}$  26.4066042155 

	top 4 features $MSE_{test}$   31.50581\\

2. Feature selection based on pearson coefficient for Iterative Residue . The selected features were ['LSTAT','PTRATIO','CHAS','RAD'] in the decreasing order of the pearson coefficent .
Selected features in the order of selection are indexed at [12, 10, 3, 8] .

	residue features MSE\_train 30.7963970728

	residue features MSE\_test 38.8714315052\\

3. Brute Force combination was observed for features indexed at [ 3, 5, 10, 12], which are ['CHAS','RM','PTRATIO','LSTAT'] \\
Best Brute Force, 

	Brute Force MSE test 35.8761588636  

	Brute Force MSE train 25.1060222464
\subsection{Polynominal Expansion }

The features were exapnded into total of 104 features using permutation of multiplication between the features. The resulted model fits the training data nicely, and does good performance on the test compared to other earlier methods.

	Poly Expansion MSE\_train = 5.63293154778

	Poly Expansion MSE\_test =  21.4246647335\\

Observation:\\
 The brute force method of feature selection is also promising, however will lead to scalablity issue, for a very large number of features and computationally heavy. The averaged Cross Validation for ridge regression performed better that the ridge regression. It can be observed that, finding the right lambda for ridge regression is not straighforward and after a certain point, we are overfitting the data, and later the MSE gap between the train and test even goes wider rather than converging to a point.
It was seen that polynominal expansion showed best performance on test set compared to other approaches., though it increases the commplexity of the model. But, it can lead to overfitting if the data is very small.
 




 



\end{document}